THE USE OF THE AREA UNDER THE ROC CURVE IN THE
EVALUATION OF MACHINE LEARNING ALGORITHMS
ANDREW E BRADLEY*
Cooperative Research Centre for Sensor Signal and Information Processing, Department of Electrical
and Computer Engineering, The University of Queensland, QLD 4072, Australia
(Received 15 April 1996; in revised form 29 July 1996; received for publication 10 September 1996)
Abstract--In this paper we investigate the use of the area under the receiver operating characteristic (ROC)
curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six
machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest
Neighbours, and a Quadratic Discriminant Function) on six "real world" medical diagnostics data sets. We
compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a
number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance
(ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision
threshold independent; and it is invafiant to a priori class probabilities. The paper concludes with the
recommendation that AUC be used in preference to overall accuracy for "single number" evaluation of machine
learning algorithms. © 1997 Pattern Recognition Society. Published by Elsevier Science Ltd.
The ROC curve
Cross-validation
The area under the ROC curve (AUC)
Wilcoxon statistic Standard error
Accuracy measures
1. INTRODUCTION
The Receiver Operating Characteristic (ROC) curve has
long been used, in conjunction with the Neyman-Pearson
method, in signal detection theory. (1'2) As such, it is a
good way of visualising a classifier's performance in
order to select a suitable operating point, or decision
threshold. However, when comparing a number of different classification schemes it is often desirable to
obtain a single figure as a measure of the classifier's
performance. Often this figure is a cross-validated estimate of the classifier's overall accuracy [probability of a
correct response, P(C)]. In this paper we discuss the use
of the area under the ROC curve (AUC) as a measure of a
classifier's performance.
This paper addresses the generic problem of how to
accurately evaluate the performance of a system that
learns by being shown labelled examples. As a case
study, we look at the performance of six different classification schemes on six "real word" medical data sets.
These data sets are chosen to characterize those typically
found in medical diagnostics, they have primarily continuous input attributes and have overlapping output
classes. When comparing the performance of the classification schemes, Analysis of Variance (ANOVA) is used
to test the statistical significance of any differences in the
accuracy and AUC measures. Duncan's multiple range (3)
test is then used to obtain the significant subgroups for
both these performance measures. Results are presented
in the form of ROC curves and ranked estimates of each
* Present address: Department of Computing Science, 615
General Services Building, University of Alberta, Edmonton,
Canada T6G 2H1. E-mail: abradley@cs.ualberta.ca.
classification scheme's overall accuracy and AUC. Discussion is then focused both on the performance of the
different classification schemes and on the methodology
used to compare them.
The paper is structured in the following way: Section 2
details some commonly used performance measures and
describes the use of the ROC curve and, in particular,
AUC as a performance measure; Section 3 briefly describes the six data sets to be used in the experimental
study; Section 4 details the implementations of the six
learning algorithms used and describes how they are
modified so that the decision threshold can be varied
and a ROC curve produced; Section 5 describes the
experimental methodology used, outlines three types
of experimental bias, and describes how this bias can
be avoided; Section 6 gives specific details of the performance measures and Section 7 the statistical techniques used to compare these measures. Section 8 presents
a summary of the results, which are then discussed in
detail in the remaining sections of the paper.
2. AUC AS A PERFORMANCE MEASURE
The "raw data" produced by a classification scheme
during testing are counts of the correct and incorrect
classifications from each class. This information is then
normally displayed in a confusion matrix. A confusion
matrix is a form of contingency table showing the
differences between the true and predicted classes for
a set of labelled examples, as shown in Table 1.
In Table 1, Tp and Tn are the number of true positives
and true negatives respectively, Fp and Fn are the numbers of false positives and false negatives respectively.
1145 
1146 A. E BRADLEY
Table 1. A confusion matrix
True class Predicted class
-ve +ve
-ve T. Fp C.
+ve F n Tp Cp
R. Rp N
The row totals, Cn and Cp, are the number of truly
negative and positive examples, and the column totals,
Rn and R e, are the number of predicted negative and
positive examples, N being the total number of examples
(N = Cn + Cp = Rn + Re). Although the confusion matrix shows all of the information about the classifier's
performance, more meaningful measures can be extracted from it to illustrate certain performance criteria,
for example:
(Tp + T~) _ P(C), (1) Accuracy (1 - Error) - (C e + Cn)
Sensitivity Tp P(T,), (2) (1 -/3) = ~ =
Specificity r. = e(r.), (3) (1 - a) =
Positive predictive value = Tp (4) R.'
T.
Negative predictive value = --. (5)
R.
All of these measures of performance are valid only for
one particular operating point, an operating point normally being chosen so as to minimise the probability of
error. However, in general it is not misclassification rate
we want to minimise, but rather misclassification cost.
Misclassification cost is normally defined as follows:
Cost = Fp . CF. ÷ r. . CFn. (6)
Unfortunately, we rarely know what the individual misclassification costs actually are (here, the cost of a false
positive, Cvp and the cost of a false negative, CF.) and so
system performance is often specified in terms of the
required false positive and false negative rates, P(Fp) and
P(F.). This then is equivalent to the Neyman-Pearson
method, (1'2) where P(Fn) is specified and P(Fp) is miniraised with that constraint, or vice versa. Often, the only
way of doing the constrained minimisation required for
the Neyman-Pearson method is to plot P(Tp) against
P(Fp) as the decision threshold is varied. Selecting the
operating point (decision threshold) that most closely
meets the requirements for P(F.) and P(Fp). The plotted
values of P(Tp) and P(Fp) as the decision threshold is
varied is called a Receiver Operating Characteristic
(ROC) curve.
There is still, however, a problem with specifying
performance in terms of a single operating point [usually
a P(Te), P(T~) pair], in that there is no indication as to
how these two measures vary as the decision threshold is
varied. They may represent an operating point where
sensitivity [P(Tp)] can be increased with little loss in
specificity [P(Tn)], or they may not. This means that the
comparison of two systems can become ambiguous.
Therefore, there is a need for a single measure of
classifier performance [often termed accuracy, but not
to be confused with P(C)] that is invariant to the decision
criterion selected, prior probabilities, and is easily extended to include cost/benefit analysis. This paper describes the results of an experimental study to investigate
the use of the area under the ROC curve (AUC) as such as
a measure of classifier performance.
When the decision threshold is varied and a number of
points on the ROC curve [P(Fp) = c~, P(Tp) = 1 -/3]
have been obtained the simplest way to calculate the area
under the ROC curve is to use trapezoidal integration,
AUC=~/{(1-/3i.Ac~)+~[A(1-~).Ac~]},
(7)
where
A(1 -/3) = (1 -/3i) - (1 - [3i-1), (8)
Aol = O~ i -- O~i_ 1 . (9)
It is also possible to calculate the AUC by assuming that
the underlying probabilities of predicting negative or
positive are Gaussian. The ROC curve will then have
an exponential form and can be fitted either: directly
using an iterative Maximum Likelihood (ML) estimation, (4) giving the difference in means and the ratio of the
variances of the positive and negative distributions; or, if
the ROC curve is plotted on double probability paper, a
straight line can be fitted to the points on the ROC
curve. (5) The slope and intercept of this fitted line are
then used to obtain an estimate of the AUC.
As noted in reference (6), the trapezoidal approach
systematically underestimates the AUC. This is because
of the way all of the points on the ROC curve are
connected with straight lines rather than smooth concave
curves. However, providing there are a reasonable number of points on the ROC curve the underestimation of the
area should not be too severe. In this experiment we
obtain at least seven points from which to estimate the
AUC and in most cases there are 15 points. The trapezoidal approach also does not rely on any assumptions as
to the underlying distributions of the positive and negative examples and, as will be elaborated on in Section 9.3, is exactly the same quantity measured using
the Wilcoxon test of ranks.
The Standard Error of the AUC(SE((J)) (6) is of importance if we wish to test the significance of one
classification scheme producing a higher AUC than
another. Conventionally there have been three ways of
calculating this variability associated with the AUC: (7)
1. from the confidence intervals associated with the
maximum likelihood estimate of AUC, (0);
2. from the standard error of the Wilcoxon statistic,
SE(W); and
3. from an approximation to the Wilcoxon statistic that
assumes that the underlying positive and negative 
The use of the area under the ROC curve in the evaluation 1147
distributions are exponential in type. (6) This assumption has been shown to be conservative; it slightly
overestimates the standard error, when compared to
assuming a Gaussian based ROC curve (as in the ML
method).
The standard error, SE(W), is given by
SE(W)
/0(1 -O) + ( C e - 1)(Q1-02)+ ( Cn -- 1)(Q2 -02)
= V
(10)
where, Cn and C v are the number of negative and positive
examples respectively and
0
Q1 - (2 - 0)' (11)
202
Q2 = (1 + 0)" (12)
In this paper we shall calculate AUC using trapezoidal
integration and estimate the standard deviation, SD(t~),
using both SE(W) and cross-validation, details of which
are given in Sections 5 and 6. Next, we shall present the
details of the data sets, classification algorithms, and
methodology chosen for this experimental study.
3. THE DATA
The data sets used in this experiment all have
two output classes and have between four and 13,
primarily continuous, input variables. Except for the
algorithms C4.5 and the Multiscale Classifier which
automatically handle categorical inputs, any categorical
input variables were made continuous by producing
dummy variables. (8)
The six data sets chosen for use in this experiment
were:
1. Cervical cell nuclear texture analysis (Texture); ~9)
2. Post-operative bleeding after cardiopulmonary bypass
surgery (Heart); 0°)
3. Breast cancer diagnosis (Breast); ~11~
4. Pima Indian's diabetes prediction (Pima); ~2~
5. Heart disease diagnosis: °3'14)
(a) Hungarian data set (Hungarian);
(b) Cleveland data set (Cleveland).
All input variables were scaled to the range [0,1 ] using
a linear transformation making the minimum value zero
and the maximum value 1. This is a requirement for the
Multiscale Classifier, (15)1 but was done for all of the
learning algorithms for consistency (with no loss of
generality). Also, all examples in the data sets that
had any missing input variables were removed; this
was less than 1% of the available data in most of the
data sets.
3.1. Cervical cell nuclear texture
These data were gathered by Ross Walker as part of a
study into the use of nuclear texture analysis for the
diagnosis of cervical cancer. (9) The data set consisted of
117 segmented images of normal and abnormal cervical
cell nuclei. Using Grey Levels Co-occurrence Matrix
(GLCM) techniques, 56 texture features were extracted
from each of these images. The six most discriminatory
features were then selected using sequential forward
selection (SFS) with the Bhattacharyya distance measure, (16"17) giving 117 examples (58 normal, 59 abnormal) each with six features:
1. Inertia at distance one;
2. Correlation at distance one;
3. Cluster prominence at distance one;
4. Entropy at distance 15;
5. Inverse Difference Moment (IDM) at distance 11;
6. Cluster prominence at distance three.
3.2. Post-operative bleeding
The data were gathered independently as part of a
study into post-operative bleeding undertaken at the
Prince Charles Hospital in Brisbane. °°) Over 200 parameters have been recorded for each of 134 patients.
However, due to the limited size of the data set, only
the four routinely measured parameters with the highest
statistical correlation to blood loss were used. z The four
parameters were
1. WBAGCOL: Aggregation with collagen (pre-operative);
2. POAGCOL: Aggregation with collagen (post-operative);
3. POSTPLT: Platelet count (post-operative);
4. DILNPLAS: Plasma dilution (post-operative).
Of the original data set of 134 patient records only 113
contained all four of the required input parameters. All of
the input parameters are continuous-valued with a lowest
possible value of zero. These parameters are then used to
predict the total blood loss, in the three hours postoperative, expressed as a ratio of body surface area.
The blood loss is then quantised into two classes, normal
and excessive bleeding. Here, a prediction of excessive
bleeding is defined as a total blood loss, in the 3 h postoperative, of greater than 16.4 ml/m 2. This defines 25%
of all patients to have bled excessively and is an arbitrary
definition that includes patients not clinically assessed as
bleeding excessively. It was necessary to associate this
absolute binary classification to the blood loss to make
the data set consistent with the others used in this paper,
lit is also recommended for methods such as k nearest ZThey were not highly correlated to the other features
neighbours.(16) selected. 
1148 A. E BRADLEY
and as part of this preliminary study, this simplistic
model was thought to be sufficient. However, most of
the classification algorithms detailed in Section 4 have
been used for regression, where the actual amount of
blood loss would be predicted quantitatively.
The remaining data sets were obtained from the
University of Southern California, machine learning
repository, ftp://ic s.uci.edu:pub/machine-leaming-databases.
3.3. Breast cancer diagnosis
Collected by Wolberg ° 1) at the University of Wisconsin, this domain contains some noise and residual variation in its 683 data points, the 16 examples with missing
attributes being removed. There are nine integer inputs,
each with a value between 1 and 10. The two output
classes, benign and malignant, are non-evenly distributed
(65.5% and 34.5% respectively).
3.4. Pima Indian's diabetes
The diagnostic, binary-valued variable investigated is
whether the patient shows signs of diabetes according to
World Health Organization criteria (i.e. if the 2 h postload plasma glucose was at least 200 mg/dl at any survey
examination or if found during routine medical care). The
population lives near Phoenix, Arizona, U.S.A. There are
eight continuously valued inputs with some noise
and residual variation. ~12) The two non-uniformly distributed output classes (65.1% and 34.9%) are tested
negative or positive for diabetes. There is a total of 768
data points.
3.5. Heart disease diagnosis
The goal of this data set is to predict the presence of
coronary artery disease from a number of demographic,
observed, and measured patient features. Here, we used
two of the available data sets (the ones with the most
instances); both data sets have the same instance format
but were collected at different hospitals.
3.5.1. Cleveland data. These data were collected by
Robert Detrano, M.D., Ph.D. at V. A. Medical centre,
The Cleveland Clinic Foundation. The data originally
were collected with 76 raw attributes; however, in
previous studies (13'14) only 14 attributes were actually
used. The data set contains 297 examples, there being
six examples removed because they had missing values.
Class distributions are 54% heart disease absent, 46%
heart disease present.
3.5.2. Hungarian data. These data were collected by
Andras Janosi, M.D. at the Hungarian Institute of
Cardiology, Budapest. The data are in exactly the same
format as the Cleveland data, except three attributes
were removed due to a large percentage of missing
values. There are 261 examples, 34 examples being
removed because they had missing values. Class
distributions are 62.5% heart disease absent, 37.5%
heart disease present.
4. THE LEARNING ALGORITHMS
The learning algorithms chosen for this experimental
comparison were:
• Quadratic Discriminant Function °8) (Bayes); 3
• k-Nearest Neighbours °9) (KNN);
• C4.5 ~2°) (C4.5);
• Multiscale Classifier O5) (MSC);
• Perceptron ~21) (PTRON);
• and Multi-layer Perceptron ~22) (MLP).
We chose a cross-section of popular machine learning
techniques together with one algorithm developed in
association with the author. There were two statistical
methods (KNN and Bayes), two neural networks
(PTRON, and MLP), and two decision trees (C4.5 and
MSC).
The following should be noted about the implementations of each of the methods, Quadratic discriminant function (Bayes). The training data are used to
estimate the prior probabilities, P(wj), mean, mj, and
covariance, Cj of the two class distributions. The Bayes
decision function for class wj of an example x is then
given by
1 1
dj(x) = lnP(wj) - ~ In Icjl - ~ [(x - mj)Tcfl(x -- mr) ].
(13)
This decision function is then a hyper-quadric, the class
of an example being selected as the minimum distance
class. Misclassification costs, cj, are then applied to these
distances, dj, so as to weight the decision function and
minimise the Bayes risk of misclassification. For these
experiments misclassification costs were used in the
range [0,1] in steps of 1/14.
k-Nearest Neighbours. For each test example, the five
nearest neighbours (calculated in terms of the sum of the
squared difference of each input attribute) in the training
set are calculated. Then, if greater than L, where
L=[0, 1, 2, 3, 4, 5], if the nearest neighbours are of class
1, the test sample is assigned to class 1; if not, it is
assigned to class 0.
Release 5 of the C4.5 decision tree generator (2°) was
used with the following modification: when pruning a
decision tree (in file prune.c) weight the local class
distributions with the misclassification costs for each
class. The default values for all parameters were used
on all the data sets. Relative misclassification costs of
[0.0:1.0, 0.015625:1.0, 0.03125:1.0, 0.0625:1.0, 0.125:-
1.0, 0.25:1.0, 0.5:1.0] were used for both classes on all
the data sets.
3We shall refer to this method as "Bayes" even though it is
not a truly Bayesian method. It would only be a Bayesian
method, i.e. optimal, if the true distributions of the input
variables were Gaussian. 
The use of the area under the ROC curve in the evaluation 1149
The Multiscale Classifier. Version 1.2bl of the Multiscale Classifier was used on each data set. The MSC was
first trained for 10 epochs, or until 100% classification
was achieved on the training set, then both pessimistic
(MSCP) and minimum error (MSCM) pruning were used
on the decision trees produced on each training set. The
default pruning parameters of cf-l% and of m=8 were
used on all data sets for pessimistic and minimum error
pruning respectively. Relative misclassification costs of
[1.0:1.0, 1.25:1.0, 1.5:1.0, 2.0:1.0, 4.0:1.0, 8.0:1.0, 16.0-
:1.0, 32.0:1.0] were used for both of the classes on all
data sets.
The Perceptron. Consisting of one neuron with a
threshold activation function. The number of inputs
(and weights) to the neuron is equal the number of input
attributes for the problem, plus a bias. The network was
trained, using the Perceptron learning algorithm (23) for
1000 epochs. The weights learnt were then tested using a
neuron with a linear activation function, scaled to give an
output in the range [0,1]. The output of this linear neuron
was then thresholded at values [0, 0.1, 0.2, 0.3 ..... 1.0]
to simulate different misclassification costs. (24)
The Multi-layer Perceptron. Three network architectures were implemented, each with different numbers of
hidden units. Their network architecture was as follows:
an input layer consisting of a number of units equal to the
number of input attributes for the problem domain; a
hidden layer consisting of 2, 4 and then 8 units; and
finally one output unit (MLP2, MLP4, and MLP8 respectively). All of the neurons were fully connected, with
log-sigmoid activation functions, i.e. their outputs were
in the range [0,1]. All three networks were trained using
back-propagation with a learning rate of 0.01, and a
momentum of 0.2. Initial values for the weights in the
networks were set using the Nguyen-Widrow method, ~25)
and the networks were trained for 20,000 epochs. Again,
during the testing phase the output neuron was thresholded at values [0, 0.1, 0.2, 0.3 .... ,1.0] to simulate
different misclassification costs. (24)
5. THE TRAINING METHODOLOGY
It is known that single train and test partitions are not
reliable estimators of the true error rate of a classification
scheme on a limited data set. (26'27) Therefore, it was
decided that a random sub-sampling scheme should be
used in this experiment to minimise any estimation bias.
A leave-one-out classification scheme was thought computationally too expensive 4 and so, in accordance with
the recommendations in reference (26), 10-fold crossvalidation was used on all of the data sets. For consistency, exactly the same data were used to train and test all
of the nine classification schemes, this is often called a
paired experimental design. (7) The 10-fold cross-validation scheme has been extensively tested and has been
shown to provide an adequate and accurate estimate of
4particularly for the Multi-layer Perceptron.
the true error rate. (27) The cross-validation sampling
technique used was random but ensured that the approximate proportions of examples of each class remain 90%
in the training set and 10% in the test set. This slight
adjustment to maintain the prevalence of each class does
not bias the error estimates and is supported in the
research literature. (26)
As pointed out by Friedman, (2s) no classification
method is universally better than any other, each method
having a class of target functions for which it is best
suited. These experiments then, are an attempt to investigate which learning algorithms should be used on a
particular subset of problems. This subset of "medical
diagnostic" problems is characterized by the six data sets
presented. Our conclusions are therefore targeted towards this subset of problems and should not be extrapolated beyond the scope of this class of problem. We
have tried to minimise any bias in the selection of the
problem domains, whilst tightly defining the subset of
problems (selection bias). We have selected problems
with a wide range of inputs (4-13) which would represent
a typical number of features measured, or feature subset
selected for medical diagnostic problems. The binary
output classes are, as would be typically expected, overlapping. This is due to varying amounts of noise and
residual variation in the measured features, and so a
100% correct classification would not, in general, be
possible.
We have tried to minimise the effect of any expert bias
by not attempting to tune any of the algorithms to the
specific problem domains. Wherever possible, default
values of learning parameters were used. These parameters include the pruning parameters for the decision
trees, the value of k for the nearest neighbour algorithm,
and the learning parameters (learning rate, momentum,
and initial conditions) for the neural networks. This naive
approach undoubtedly results in lower estimates of the
true error rate, but it is a bias that affects all of the
learning algorithms equally. If we had attempted to tune
the performance of each algorithm on each data set, then
our different expertise with each method would of advantaged some algorithms but disadvantaged others. The
experimentation time would also have increased dramatically as we evaluated different input representations,
input transformations, network architectures, learning
parameters, pruning parameters, or identified outlying
examples in the training set. Also, in domains with a
limited availability of data the introduction of an evaluation set (extracted from the training set) could actually
reduce the overall accuracy of the algorithms.
6. THE PERFORMANCE MEASURES
For each learning algorithm (9 off) on each data set (6
off), 10 sets of results (one for each of the 10-fold crossvalidation partitions) were stored. The raw data were
stored in the form of a confusion matrix and for each of
the 10 test partitions the decision thresholds were varied
(to produce the ROC curves), giving between 7 and 15 
1150 A. R BRADLEY
sets of results for each test partition. In order to
evaluate the performance of the different learning algorithms on each of the data sets, a number of measures
have to be extracted from this raw data (over 6000 sets of
results).
Overall accuracy, P(C). For the default (conventional)
decision thresholds, with equal misclassification costs,
the value of the estimate of the true error rate [equation (1)] was calculated for the 10 cross-validation partitions.
The ROC curve. On each test partition the decision
thresholds were effectively varied (by varying
misclassification costs, as described in Section 4) to give
a set of values for P(Tp) and P(Fp). The "average"
ROC curves for each classification scheme are shown
in Section 8.
The area under the ROC curve (AUC). As the
misclassification costs were varied, as described in
Section 4, each successive point on the ROC curve
was used in the trapezoidal integration to calculate
AUC. The AUC was calculated for each learning algorithm on each of the 10 test partitions. This is in effect
using a jackknife estimate to calculate the standard error
of the AUC (29) and will be discussed in more detail
shortly.
Remark. It should be noted that there are two distinct
possibilities when it comes to combining the ROC curves
from the different test partitions, (3°)
1. Pooling. In pooling, the raw data (the frequencies of
true positives and false positives) are averaged. In
this way one average, or group ROC curve is
produced from the pooled estimates of each point
on the curve. In this case we have 10 estimates of
P(Tp) and P(Fp) for each point on the ROC curve.
The assumption made when pooling the raw data is
that each of the classifiers produced on each of the
training partitions comes from the same population.
Although the assumption that they come from the
same population may be true in terms of their overall
discrimination capacity (accuracy), the assumption
that for each partition they are all estimating the
same points on the ROC curve is less palatable. This
can be seen from the fact that pooling the data in this
way depresses the combined index of accuracy,
AUC.(3°)
2. Averaging. This alternative approach is to average
the accuracy index extracted from each of the ROC
curves on the 10 train and test partitions. So, AUC
is calculated for the 10 ROC curves and then
averaged, giving an estimate of the true area and an
estimate of its standard error, calculated from the
standard deviation of the 10 areas. The only problem
with this approach is that it does not result in an
average ROC curve, only an average AUC. For this
reason the pooled responses are used when actually
visually showing the whole ROC curves, as in
Section 8.
The standard deviation of AUC, SD(0). In order to
validate our estimate of the standard deviation of AUC
obtained using averaging, SD(0), SE(W) was also calculated using the approximation to the Wilcoxon method,
given in equation (10).
7. THE COMPARATIVE TECHNIQUES
7.1. Analysis of variance
In this paper we will use Analysis of Variance (ANOVA) techniques to test the hypothesis of equal means over
a number of learning algorithms (populations) simultaneously. (3) The experimental design allows us to compare, on each data set, the mean performance for each
learning algorithm and for each train and test partition.
This is called two-way classification and effectively tests
two hypotheses simultaneously:
1. H i, that all the means are equal due to the different
train and test partitions;
2. H~, that all the means are equal due to the different
learning algorithms.
Of these two hypotheses we are only really interested in the second, H~, and we could have used a
one-way ANOVA to test this hypothesis alone. However,
a one-way ANOVA assumes that all the populations
are independent, and can often be a less sensitive test
than a two-way ANOVA, which uses the train and
test partitions as a blocking factor. (31) The f ratio
calculated from this ANOVA table is insensitive to
departures from the assumption of equal variances when
the sample sizes are equal, as in this case. O) For this
reason a test for the equality of the variances was not
done.
7.2. Duncan's multiple range test
When the analysis of variance test on an accuracy
measure produces evidence to reject the null hypotheses, H~ and H~, we can accept the alternative hypothesis-that all of the mean accuracies are not
equal. However, we still do not know which of the means
are significantly different from which other means, so we
will use Duncan's multiple range test to separate significantly different means into subsets of homogeneous
means.
For the difference between two subsets of means to be
significant it must exceed a certain value. This value is
called the least significant range for the p means, Rp, and
is given by
Rp = rp V/~, (14)
where the sample variance, s 2, is estimated from the
error mean square from the analysis of variance, s~, r
the number of observations (rows), and rp the least
significant studentized-range for a given level of
significance (we chose ct=0.05), and the degrees of
freedom [(r-1)(c- 1)= 721. Tables 2-7 show the
subsets of adjacent means that are not significantly
different, this being indicated by drawing a line under
the subset. 
The use of the area under the ROC curve in the evaluation 1151
8. RESULTS
In this section we give the summary of the results.
• Nuclear Texture: See Table 2 and Figs 1 and 2.
• Post-operative Heart Bleeding: See Table 3 and Figs 3
and 4.
• Breast Cancer: See Table 4 and Figs 5 and 6.
• Pima Indians Diabetes: See Table 5 and Figs 7 and 8.
• Cleveland Heart Disease: See Table 6 and Figs 9 adn
10.
• Hungarian Heart Disease: See Table 7 and Figs 11
and 12.
Table 2. Rank ordered significant subgroups from Duncan's multiple range test on the nuclear texture data
Classifier: PTRON MSCM MSCP C4.5 KNN BAYES MLP8 MLP4 MLP2
Accuracy: 85.0 85.0 85.0 89.2 89.2 89.2 90.0 90.0 91.7
Classifier: MSCP MSCM C4.5 KNN BAYES PTRON MLP4 MLP8 MLP2
AUC: 88.1 88.7 92.1 96.2 96.7 97.8 98.3 98.5 98.6
Table 3. Rank ordered significant subgroups from Duncan's multiple range test on the heart bleeding data
Classifier: MSCM MSCP C4.5 PTRON KNN MLP8 MLP4 MLP2 BAYES
Accuracy: 69.2 70.8 71.7 72.5 74.2 75.0 76.7 78.3 79.1
Classifier: C4.5 KNN MLP4 MLP8 MLP2 PTRON MSCM MSCP BAYES
AUC: 48.7 60.9 65.5 65.7 66.1 69.8 70.0 70.5 73.3
Table 4. Rank ordered significant subgroups from Duncan's multiple range test on the breast cancer data
Classifier: PTRON C4.5 MSCM MSCP MLP8 MLP4 MLP2 KNN BAYES
Accuracy: 72.2 90.7 90.9 91.2 92,7 93.3 93.5 93.6 94.2
Classifier: C4.5 MSCM MSCP PTRON MLP4 MLP8 MLP2 KNN BAYES
AUC: 93.7 94.4 94.4 94.5 95.2 96.2 96.5 97.0 98.2
Table 5. Rank ordered significant subgroups from Duncan's multiple range test on the Pima diabetes data
Classifier: MSCM MSCP C4.5 PTRON KNN BAYES MLP8 MLP4 MLP2
Accuracy: 68.1 68.2 71.7 73.6 74.8 75.9 77.0 77.1 78.4
Classifier: MSCM MSCP BAYES KNN C4.5 MLP8 MLP4 PTRON MLP2
AUC: 74.1 74.4 76.3 79.4 80.2 82.3 83.4 84.7 85.3 
1152 A.P. BRADLEY
Table 6. Rank ordered significant subgroups from Duncan's multiple range test on the Cleveland heart disease data
Classifier: MSCM MSCP PTRON C4.5 MLP8 MLP4 MLP2 KNN BAYES
Accuracy: 68.7 68.7 75.0 77.7 81.0 81.0 81.3 82.7 86.3
Classifier: MSCP MSCM C4.5 MLP8 MLP2 MLP4 KNN BAYES PTRON
AUC: 73.7 73.8 84.2 84.4 85.9 86.1 86.9 90.8 91.2
Table 7. Rank ordered significant subgroups from Duncan's multiple range test on the Hungarian heart disease data
Classifier: MSCM MSCP C4.5 KNN MLP4 PTRON MLP8 BAYES MLP2
Accuracy: 71.5 71.5 73.0 74.1 75.5 76.7 77.4 78.9 79.3
Classifier: MSCM MSCP C4.5 KNN MLP8 MLP4 BAYES MLP2 PTRON
AUC: 70.1 70.2 79.2 82.0 82.1 82.3 83.8 84.7 87.8
ROC Cuwe ROC Cu~e
0.9 0.9
0.8 0.8
2i.! t g
o~ o21f// / • Perceptron o /o V
P(Falee Positive) (Alpha) P(False Positive) (Alpha)
Fig. 1. ROC curve for Bayes, KNN, and MLP on the nuclear Fig. 2. ROC curve for C4.5, MSC, and Perceptron on the
texture data. nuclear texture data.
9. DISCUSSION
In this section we discuss only the second hypothesis
tested by the two-way analysis of variance (ANOVA),
H~. This is the variance due to the different learning
algorithms (column effects). The reason for this is that
the train and test partitions are being used as what is
called a blocking factor. We would hope for a significant
effect due to the train and test partitions, 5 not because this
5So that we can reject H~.
variance is of any scientific interest, but because it is
necessary for the two-way ANOVA to be more efficient
than the one-way ANOVA.
9.1. Overall accuracy
All of the data sets showed some difference in average
accuracy for each of the learning algorithms. However,
the ANOVA showed that on one of these data sets
(Nuclear Texture) there was no significant evidence
(p < 0.05) for the mean accuracies to be actually differ- 
The use of the area under the ROC curve in the evaluation 1153
0.!
0.8
~0.7
6o.6
~ 0.5
o.
~ 0.4
"~ 0.3
0.2
0.1
ROC Curve
01 02 03 04 05 01.6 01.7 01,8 01.9
P(False Positive) (Alpha)
ROC Curve
0.9
0.8
0.7
0.6
0.5
0.4
?.1
i i i i P i i i
0 0.1 0.2 0.3 0.4 0,5 0.6 0,7 0.8 0.9 1
P(False Positive) (Alpha)
Fig. 3. ROC curve for Bayes, KNN, and MLP on the heart Fig. 4. ROC curve for C4.5, MSC, and Perceptron on the heart
bleeding data. bleeding data.
1
0.9
0.8
~0.7
®
0.6
:~0.5
o.
~ 0.4
0.3
0.2
0.1c
ROC Curve
o'1 °'2 o'3 o', ols ols o17 oi~ o'9
P(False Positive) (Alpha)
Fig. 5. ROC curve for Bayes, KNN, and MLP on the breast
cancer data.
ROC Curve
i
P(False Positive) (Alpha)
Fig. 6. ROC curve for C4.5, MSC, and Perceptron on the breast
cancer data.
ent. On the other five data sets (where there was significant evidence to reject the null hypothesis, I~)
Duncan's multiple range test was used to find the significant subgroups.
The Post-operative heart bleeding data set shows only
two significant subgroups. Table 3 also shows that there
is only a significant difference between the two decision
trees methods (MSC and C4.5) and the MLP with two
and four hidden units and Bayes.
Table 4 shows that for the Breast Cancer data set there
are three significant subgroups: one subgroup containing
only the Perceptron; one containing the two decision
trees (MSC and C4.5); and the other learning algorithms
in the third. There is also an overlap between the last two
groups as the number of hidden units in the MLP is
increased above 2. The fact that the Perceptron is in the
lowest subgroup on its own would indicate that this
problem is not linearly separable and so the Perceptron
lacks the representation power to achieve a high overall
accuracy. In addition, the lower performance observed
using the decision tree methods may indicate that the
optimal decision surface is smooth in nature.
The Pima Indians diabetes data set (Table 5) shows
three significant subgroups under overall accuracy. The
lowest accuracy group contains the decision trees (MSC
and C4.5) though only Bayes and the Multi-layer Perceptrons (MLP) have a significantly (p < 0.05) higher
overall accuracy. The poor performance of the decision 
1154 A.P. BRADLEY
ROC Curve ROC Curve
0.9 0.9
0.8 0.8
PI /7 / /t/ /
i o.~ o,ry / ,'"',j / ! ~-o.314¢/y, / + -°
Ill I / . Pe,:,,oo
0.2 0,2I/I/ / -- --
0. 0.1
014 015 016 017 018 019 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 ' -' '- -' '- 0.8 ' ' ' 0.9 1 P(False Positive) (Alpha) P(False Positive) (Alpha)
Fig. 7. ROC curve for Bayes, KNN, and MLP on the Pima Fig. 8. ROC curve for C4.5, MSC, and Perceptron on the Pima
diabetes data. diabetes data.
ROC Curve ROC Curve
0.9 0.9
0.8 0.8
Om7 ~0"7
!t// / 06 -0.6
Om5~ ~/ I / I + ~-- I t "~ ~ 0.5
~ 0m4 ~ 014 -- ¢
:'l// / t °"0"3 0.2 ~ " + Perceptron MSC
0.~ 0.1
~0 011 012 013 014 0'5 016 017 018 019 i P(False Positive) (Alpha) P(False Positive) (Alpha)
Fig. 9. ROC curve for Bayes, KNN, and MLP on the Cleveland Fig. 10. ROC curve for C4.5, MSC, and Perceptron on the
heart disease data. Cleveland heart disease data.
trees may indicate that the smooth decision hyperplanes
are perhaps better suited to this problem, especially with
the limited training data available. The relative success of
the MLPs over the Bayesian method would indicate that
the input features are not Normally distributed and so the
covariance matrix is not being reliably calculated.
From Table 6, it can be seen that the Cleveland heart
disease data set has four significant subgroups under
overall accuracy. However, due to the large amount of
subgroup overlap, there seems to be little discrimination
due to the classification method. Perhaps of note, though,
is the fact that on this problem the Bayes and KNN
methods obtained the highest overall accuracies. This
was surprising because the number of input features is 13,
it being considered that when you have more than 10
input features the curse of dimensionality will start
having a major effect. (8) Of all the learning algorithms
used in this experiment, one would expect the Bayes and
KNN to be the most severely affected by the curse of
dimensionality. However, on this domain, this was obviously not the case.
Table 7 shows two significant subgroups for overall
accuracy on the Hungarian heart disease data set. However, both of these subgroups are widely overlapping, the
only significant differences being between the MSC and
both the Bayes and the MLP (with two hidden units).
In general, when performance is measured in terms of
overall accuracy, the hyper-plane (Bayes and MLP) and 
O.E
A0.7
- 0.13 v
0.
~o4
9.3
0.2
9.1
o11 o12 0'.3 01, o15 0'.6 017 o18 0'.9
P(False Positive) (Alpha)
Fig.
ROC Curve
The use of the area under the ROC curve in the evaluation 1155
ROC Curve
O.,C
01
05
0.(
0.,'
0.4
o C4.5
0.3
0.2 on
3.1
l
(] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 
1156 A.P. BRADLEY
range test was carried out on all six data sets to determine
the significant subgroups.
On the nuclear texture data set, three significant subgroups were obtained, as shown in Table 2. The decision
trees (MSC and C4.5) are in a lower performance subgroup of their own, with C4.5 in a second subgroup with
KNN, and Bayes, the third, highest performance group,
now includes the Perceptron and Multi-layer Perceptrons
but excludes the decision trees (C4.5 and MSC). The poor
performance obtained using the decision tree methods
can be attributed to the fact there are limited data with
which to construct and prune the trees and that smooth
decision hyper-planes are probably more suitable than
hyper-rectangles in this problem domain. Of note is the
fact that the Perceptron and MSC obtained the same
accuracy, P(C), but the Perceptron now has a significantly higher (p < 0.05) AUC. With that exception there
is an extremely good correlation between the rankings
given from P(C) and that given from AUC. However,
AUC produced significant differences between the mean
performance, whereas P(C) did not.
There are two significant subgroups for the postoperative bleeding data set, as shown in Table 3. The
lowest performance subgroup contains C4.5 only, the
other subgroup containing all of the other methods. The
low performance of C4.5 when measured using AUC can
also be visually seen in the ROC curves of Figs 3 and 4.
In this data set there are patients who have bled excessively due to a surgically related complication (a technical error). Some of the training data have therefore
effectively been misclassified because the excessive
bleeding was not related to any of the features measured,
but was a consequence of the technical error. These cases
should randomly affect the data and therefore become
isolated examples in feature space. We would hope that
they would have little effect on the classifier during
training, but this will be dependent on the classification
algorithm used. The effect of these points on the MLP,
Perceptron, and Bayes methods is to bias the position of
the decision boundary(s); however, if, as is thought for
this case, the number of misclassified points is not too
large, this effect should be minimal. KNN will be affected dependent upon the amount of smoothing built
into the classification, i.e. upon the choice of K. For the
decision tree methods (C4.5 and MSC) these points will
cause the formation of erroneous decision nodes in the
tree. However, it should then be possible to prune these
examples from the tree to eliminate their effect, as they
will be nodes that have seen very few training points, i.e.
they have a low confidence level. However, because of
the lack of data in this domain it is very difficult to
determine with certainty which data points are due to a
technical error and therefore should be pruned and which
data points are due to the underlying problem. This can
be seen in Fig. 4 particularly in the cases of the decision
tree C4.5 where the pruning has reduced the structure of
the tree too much and hence reduced the sensitivity. This
means that C4.5 is very rarely predicting any cases as
being positive; this "over caution" leads to what appears
to be a acceptable accuracy, but a very low AUC. This
means that the decision tree is actually doing very little
work. In previous experiments (32) we found that the MSC
obtained a higher accuracy (76%) when no pruning was
done on the tree. This is an example of a problem domain
where the algorithm has been biased by the decision tree
pruning. (33)
There are three significant subgroups shown for the
Breast Cancer data set in Table 4. There is a large amount
of overlap in these subgroups and so no real identifiable
groups seem to exist. However, there is an indication of a
general increase in performance from the decision trees
through the Perceptron on to the MLPs and then up to the
KNN and Bayes methods. Again, with the exception of
the Perceptron, which again obtained a higher ranking of
performance under AUC than it did under P(C), there is
good agreement in the ranking between the two performance measures.
Table 5 shows that for the Pima Indians Diabetes data
set there are four significant subgroups (as compared to
three for overall accuracy). This again would indicate the
increased sensitivity of AUC over P(C) as a measure of
classifier performance. In fact, it may well be worth
going to a higher level of significance (say p=0.01) to
reduce the number of subgroups and reveal a more
general underlying trend. In addition, it can be seen from
the ROC curve for the Bayes classifier (Fig. 7) that there
are only really three points from which to estimate the
AUC. This means that the AUC calculated for the Bayes
classifier on this data set will be pessimistically biased.
To avoid this effect it may be possible to implement a
systematic way of varying the decision threshold when
producing the ROC curves, rather than using linear
steps. (34)
The Cleveland heart disease data set has three significant subgroups of performance under AUC (see
Table 6). The MSC is in a subgroup of its own, the other
two groups being fairly overlapping and so no meaningful subgroups can be identified. Again, the Perceptron
obtained a higher ranking under AUC than it did under
overall accuracy. With this exception, there is a good
level of agreement in the ranking of the performance of
the classification algorithms under accuracy and AUC.
Where accuracy found two broad significant subgroups, Table 7 shows that AUC has produced three
subgroups on the Hungarian Heart Disease data set.
The MSC is in the lowest performance subgroup (on
its own) while the remaining two subgroups are broadly
overlapping with only a significant difference between
the AUC for C4.5 (lowest) and the Perceptron (highest).
As was the case for the Cleveland heart disease data set,
the Perceptron performed better under AUC than it did
under overall accuracy, but otherwise accuracy and AUC
produced similar rankings of performance.
9.3.1. The meaning of AUC. It may seem that
extracting the area under the ROC curve is an
arbitrary feature to extract. However, it has been
known for some time that this area actually represents
the probability that a randomly chosen positive example
is correctly rated (ranked) with greater suspicion than a 
The use of the area under the ROC curve in the evaluation 1157
0.3 i , i i i
0.25 x
x~ x
.~ 0.;
× × D
~0.15
~0.1
o.o,
AUC Standard Error
Fig. 13. Scatter plot of the standard error of the Wilcoxon
statistic versus the standard deviation of the AUC. There are
nine learning algorithms, each data set being shown with a
different tick mark.
0.35 .....
/ /
0.3 ,,,"
O.25
0.2 ,"
D.15 ,'/ ~
//1
0.1 ~
~.o~, ~ -
-o.o5 o.'o o.' 5
AUG Standard Error
Fig. 14. Linear relationship between the standard error of the
Wilcoxon statistic and the standard deviation of the AUC.
randomly chosen negative example. (6) Moreover, this
probability of correct ranking is the same quantity
estimated by the Wilcoxon statistic. (6'35)
The Wilcoxon statistic, W, is usually used to test the
hypothesis that the distribution of some variable, x, from
one population (p) is equal to that of a second population
(n), H0 : Xp = Xn .(3) If this (null) hypothesis is rejected
then we can calculate the probability, p, that Xp > xn,
Xp < xn, or Xp ¢ xn. In our case, where we want good
discrimination between the populations p and n, we
require P(xp > xn) to be as close to unity as possible.
The Wilcoxon test makes no assumptions about the
distributions of the underlying populations and can work
on continuous, quantitative, or qualitative variables.
As already discussed AUC effectively measures
P(xp > xn). In the same situation, given one normal
example and one positive example, 6 a classifier with
decision threshold t will get both examples correct with a
probability,
P(C) = P(xp > t)P(x. < t). (15)
P(C) is dependent on the location of the decision threshold t and is therefore not a general measure of classifier
performance.
9.3.2. The standard error of AUC. The AUC, 0, is an
excellent way to measure P(xp >xn) for binary
classifiers and the direct relationship between, W, and
0 can be used to estimate the standard error of the AUC,
using SE(W) in equation (10).
Figures 13 and 14 show how the standard error of the
Wilcoxon statistic, SE(W), is related to the standard
6Often referred to as a two alternative forced choice
experiment (2AFC).
deviation of the averaged AUC, SD(0), calculated using
10-fold cross-validation. The correlation coefficient between SE(W) and SD(0) is 0.9608 which indicates that
there is a very strong linear relationship between SE(W)
and SD(0). Over all six data sets, SE(W) has a mean value
of 0.0770 and a standard deviation of 0.0482, whilst
SD(0) has mean 0.0771 and standard deviation 0.0614.
This again would indicate that although SD(0) has a
higher variance it is a very good estimator of SE(W). The
straight line fitted (in a least squared sense) to SE(W) and
SD(0) in Fig. 14 again reiterates the quality of SD(0) as
an estimate of SE(W).
The larger variance observed for SD(0) can be explained when you consider the fact that SD(0) has two
sources of variance. The first source of variance, which is
also the variance estimated by SE(W), is due to the
variation of the test data. That is, in each of the 10
iterations of cross-validation there is a different 10% of
the data in each test partition. These different sets of test
data therefore produce different ROC curves, and AUC
varies accordingly. The second source of variance is due
to variation of the training data in each cross-validation
partition. The variation in the training data used in each
cross-validation partition also affect the ROC curves
produced and this causes AUC to vary. However, because
only one-ninth of the training data vary with each subsequent training partition, this second source of variance
is small and therefore, as was shown, SD(0) is a good
estimator of SE(W).
Figure 15 shows how the standard error of the Wilcoxon statistic varies with the number of test samples and
the actual value of the AUC. The two trends to notice are:
1. As the number of test samples increase the standard error decreases, SE(W) being inversely pro- 
1158 A.P. BRADLEY
0.12
0.1
~ 0.08
t~ 0.06
x~
0.04
0.02
-- AUC = 095
- - AUC = 0.85
• AUC = 0.75
::_::: ......................
i i i
°0 2'0 4'0 0'0 ,0 1~0 ,~0 1~0 180 180 200
Number of Abnormal Examples, Na ( = Nn)
Fig. 15. Variation of the standard error of the Wilcoxon
statistic with AUC and the number of test examples, assuming
c.=cp.
portional to x/N, where N is the number of test
samples•
2. SE(W) is inversely proportional to AUC. There is a
high variance associated with small values of AUC
(< 0.8) and the variance becomes very small as the
AUC gets close to 1. This effect can also be seen in
Fig. 13; the "x" points represent the standard
error and deviation estimated for the heart bleeding
domain. On this domain the AUC was quite low
(~0.66) and so the variation is noticeably higher.
There are also methods to reduce the standard error
estimate for classifiers tested on the same data, ~7> with its
own significance test (to compare two AUCs). There are
other measures of performance such as output signal-tonoise ratio, or deflection criterion, O6) but the AUC seems
to be the only one that is independent of the decision
threshold and not biased by prior probabilities.
• It gives an indication of how well separated the
negative and positive classes are for the decision index,
e(xp > Xn);
• It is invariant to prior class probabilities•
• Itgivesanindicationoftheamount of "work done" by
a classification scheme, giving low scores to the random or "one class only" classifiers•
However, there was good agreement between accuracy
and AUC as to the ranking of the performance of the
classification algorithms. It was also found that the
standard deviation of the averaged AUCs from the 10-
fold cross-validation can be used as an estimate of the
standard error of the AUC calculated using the approximation to the Wilcoxon statistic.
The results quoted for the all the algorithms are only
valid for the particular architecture or parameter settings
tested, there may be other architectures that offer better
performance. However, care should be taken when
choosing parameters so as not to optimistically bias
the results• Using a training, evaluation, and test set
methodology should prevent this. Finally, for one particular application, the best way to select a classifier and its
operational point is to use the Neyman-Pearson method,
of selecting the required sensitivity and then maximising
the specificity with this constraint (or vice versa). The
AUC however, appears to be one of the best ways to
evaluate a classifier's performance on a data set when a
"single number" evaluation is required or an operational
point has not yet been determined.
Acknowledgements--The Author is grateful to Geoffrey
Hawson and Michael Ray of the Prince Charles in Brisbane
for allowing access to the post-operative heart bleeding data set
used in this study. The work of Michael Ray and Geoffrey
Hawson is kindly supported by the Prince Charles Hospital
Private Practice Study, Education, and Research Trust Fund.
Thanks are also due to Gary Glonek, Brian Lovell, Dennis
Longstaff, and the anonymous referees for helpful comments
on earlier drafts of this paper.
I0. CONCLUSIONS
In general there was not a great difference in the
accuracies obtained from each of the learning algorithms
over all the data sets. Generally, the hyperplane (Bayes,
MLP) and exemplar (KNN) based methods performed
better than the decision trees (C4.5, MSC) in terms of
overall accuracy and AUC. However, this is due, in part,
to the type of problems we have analysed, being primarily continuous inputs with overlapping classes; the models used by these methods are particularly well suited to
this type of problem.
The area under the ROC curve (AUC) has been shown
to exhibit a number of desirable properties as a classification performance measure when compared to overall
accuracy:
• Increased sensitivity in the Analysis of Variance
(ANOVA) tests;
• It is not dependent on a decision threshold chosen;
