Joint Sentiment/Topic Model for Sentiment Analysis
Chenghua Lin School of Engineering, Computing and Mathematics University of Exeter North Park Road, Exeter EX4 4QF, UK cl322@exeter.ac.uk
Yulan He Knowledge Media Institute The Open University Milton Keynes MK7 6AA, UK y.l.he.01@cantab.net
ABSTRACT Sentiment analysis or opinion mining aims to use automated tools to detect subjective information such as opinions, attitudes, and feelings expressed in text. This paper proposes a novel probabilistic modeling framework based on Latent Dirichlet Allocation (LDA), called joint sentiment/topic model (JST), which detects sentiment and topic simultaneously from text. Unlike other machine learning approaches to sentiment classi?cation which often require labeled corpora for classi?er training, the proposed JST model is fully unsupervised. The model has been evaluated on the movie review dataset to classify the review sentiment polarity and minimum prior information have also been explored to further improve the sentiment classi?cation accuracy. Preliminary experiments have shown promising results achieved by JST.
Categories and Subject Descriptors I.2.7 [Arti?cial Intelligence]: Natural Language Processing—Text analysis
General Terms Algorithms, Experimentation
Keywords Sentiment analysis, Opinion mining, Latent Dirichlet Allocation, Joint sentiment/topic model
1. INTRODUCTION As propelled by the rapid growth of text data, text mining has been applied to discover hidden knowledge from text in many applications and domains. In business sectors, great e?orts have been made to ?nd out customers’ sentiments and opinions, often expressed in free text, towards companies’ products and services. However, discovering sentiments and opinions through manual analysis of a large volume of textual data is extremely di?cult. Hence, in recent years, there
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro?t or commercial advantage and that copies bear this notice and the full citation on the ?rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci?c permission and/or a fee. CIKM’09, November 2–6, 2009, Hong Kong, China. Copyright 2009 ACM 978-1-60558-512-3/09/11 ...$10.00.
have been much interests in the natural language processing community to develop novel text mining techniques with the capability of accurately extracting customers’ opinions from large volumes of unstructured text data. Among various opinion mining tasks, one of them is sentiment classi?cation, i.e. whether the semantic orientation of a text is positive, negative or neutral. When applying machine learning to sentiment classi?cation, most existing approaches rely on supervised learning models trained from labeled corpora where each document has been labeled as positive or negative prior to training. Such labeled corpora are not always easily obtained in practical applications. Also, sentiment classi?cation models trained on one domain might not work at all when moving to another domain. Furthermore, in a more ?ne-grained sentiment classi?cation problem (e.g. ?nding users’ opinions for a particular product feature), topic/feature detection and sentiment classi?cation are often performed in a two-stage pipeline process, by ?rst detecting a topic/feature and later assigning a sentiment label to that particular topic. Intuitively, sentiment polarities are dependent on topics or domains. Therefore, detecting both topic and sentiment simultaneously should serve a critical function in helping users in terms of opinion mining and summarization. For instance, though the adjective ‘unpredictable’ in a phrase such as ‘unpredictable steering’ may have negative orientation in an automobile review, it could also have positive orientation in a phrase like ‘unpredictable plot’ in a movie review [5]. Although much work has been done in detecting topics [2, 6, 20], these lines of work mainly focused on discovering and analyzing topics of documents alone, without any analysis of sentiment in the text, which limit the usefulness of the mining results. Other work [16, 22, 11, 15, 4, 3, 25] addressed the problem of sentiment detection in various levels (i.e. from word/phrase level, to sentence and document level). However, none of them can model mixture of topics alongside with sentiment classi?cation, which again makes the results less informative to users. Some of the recent work [14, 19] has been aware of this limitation and tried to capture sentiments and mixture of topics simultaneously. However, Mei et al. [14] does not model sentiment directly and requires post-processing to calculate the positive/negative coverage in a document in order to identify its polarity. Titov and McDonald [19] requires some kind of supervised settings that the customer reviews should contain ratings for the aspects/features discussed in the text and thus it lacks of the ?exibility to adapt to other domains.
In this paper, we focus on document level sentiment classi?cation based on the proposed unsupervised joint sentiment/topic (JST) model. This model extends the stateof-the-art topic model, Latent Dirichlet Allocation (LDA), by adding a sentiment layer. Our model distinguishes from other models in that: (1) JST is fully unsupervised; (2) JST can detect sentiment and topic simultaneously. To the best of our knowledge, no other existing approaches present the same merits as our model. We have also explored various approaches for obtaining prior information in order to improve the sentiment detection accuracy. Although the proposed JST model can be easily extended to detect polarity of text at various granularity levels, in this paper we mainly focus on reporting our preliminary results on the document-level sentiment classi?cation and brie?y present the sentiment analysis results on some extracted topics as an example illustration. The rest of the paper is organized as follows. Section 2 introduces the related work. Section 3 presents the Joint Sentiment/Topic (JST) model. We show the experimental setup in Section 4 and discuss the results based on the movie review dataset1 in Section 5. Finally, Section 6 concludes the paper and outlines the future work.
2. RELATED WORK Great bulk of work has been focused on the problem of sentiment classi?cation at various levels using machine learning techniques. Turney and Littman [22] applied an unsupervised learning algorithm to classify the semantic orientation in the word/phrase level, based on mutual information between document phrases and a small set of positive/negative paradigm words like “good” and “bad”. Choi et al. [4] dealt with opinion analysis by combining conditional random ?elds (CRFs) and a variation of Autoslog. In the sentence level, a semi-supervised machine learning algorithm was proposed by Pang and Lee [15], which employs a subjectivity detector and minimum cuts in graphs. Another system by Kim and Hovy [11] judges the sentiment of a given sentence by combining the individual word-level sentiment. Eguchi and Lavrenko [5] proposed a generative model that jointly models sentiment words, topic words and sentiment polarity in a sentence as a triple. In more recent work [25], the authors tackled this problem utilizing CRFs and considered both contextual dependency and label redundancy in sentence sentiment classi?cation. Another line of work is in the document level, where one tries to evaluate the overall sentiment of a document. The representative work at the early stage can be found in [21, 16], where the former used unsupervised learning and mutual information, which is similar to the approach proposed in [22]; while the latter classi?ed the polarity of movie reviews with the traditional supervised text categorization methods. Following this way, lots of other approaches have been proposed. For example, McDonald et al. [13] investigated a global structured model that learns to predict sentiment of di?erent levels of granularity in text. Blitzer et al. [3] focused on domain adaption for sentiment classi?ers with respect to di?erent types of products’ online reviews. However, as can be easily pointed out, all the aforemen
1Polarity dataset v2.0 URL: http://www.cs.cornell.edu/people/pabo/movie-reviewdata/
tioned work shares some similar limitations: (1) they only focus on sentiment classi?cation without considering the mixture of topics in the text, which is less informative to users and may limit the usefulness of the results; (2) most of the approaches [16, 15, 4, 3, 13, 25] are favored in supervised learning, which require a labeled corpus for training and potentially restrain their applicability to other domains of interest. Motivated by these observations, we construct an unsupervised hierarchical Bayesian model which can classify document level sentiment and extract mixture of topics simultaneously. To the best of our knowledge, not much work has been done regarding this particular problem. However, there are indeed several lines of work which are quite close to our vision [14, 20, 19]. One of the most closely related work is the Topic-Sentiment Model (TSM) [14], which jointly models the mixture of topics and sentiment predictions for the entire document. However, there are several intrinsical di?erences between JST and TSM. First of all, TSM is essentially based on the Probabilistic Latent Semantic Indexing (pLSI) [8] model with an extra background component and two additional sentiment subtopics, and thus su?ers from the problems of inferencing on new document and over?tting the data, both of which are known as the de?cits of pLSI. JST overcomes these shortcomings as it is based on LDA with a better statistical foundation. Regarding topic extraction, TSM samples a word either from the background component model or topical themes where the latter are further categorized into three sub-categories, i.e. neutral, positive and negative sentiment models. In contrast, in JST one draws a word from the distribution over words jointly de?ned by topic and sentiment label that chosen in the ?rst place. Thirdly, for sentiment detection, TSM requires postprocessing to calculate the sentiment coverage of a document, while in JST the document sentiment can be directly obtained from the probability distribution of sentiment label given document. Other models by Titov and McDonald [20, 19] are also closely related to ours, since they are all based on the stateof-the-art topic model LDA. First proposed in [20], the MultiGrain Latent Dirichlet Allocation model (MG-LDA) is argued to be more appropriate to build topics that are representative of ratable aspects of objects from online user reviews, by allowing terms being generated from either a global topic or a local topic. Being aware of the limitation that MG-LDA is still purely topic based without considering the associations between topics and sentiments, Titov and McDonald further proposed the Multi-Aspect Sentiment model (MAS) [19] by extending the MG-LDA framework. The major improvement of MAS is that it can aggregate sentiment texts for the sentiment summary of each rating aspect extracted from the MG-LDA. Our model di?ers from MAS in several aspects: MAS works on a supervised setting as it requires that every aspect is rated at least in some documents, which is practically infeasible in real life applications, while our JST model is fully unsupervised with only minimum prior information being incorporated, which in turn has more ?exibilities; MAS focuses on extracting text for sentiment summaries of each aspect ratings while we predict the sentiment orientation in the document level.
3. JOINT SENTIMENT/TOPIC (JST) MODEL The Latent Dirichlet Allocation (LDA) model, as shown

w



z NdDT (a)


w
 z lNd DSS*T (b)

w
  z lNd DSS*T (c) Figure 1: (a) LDA model; (b) JST model; (c) Tying-JST model. in Figure 1(a), is one of the most popular topic models based upon the assumption that documents are mixture of topics, where a topic is a probability distribution over words [2, 18]. The LDA model is e?ectively a generative model from which a new document can be generated in a prede?ned probabilistic procedure. Compared to another commonly used generative model Probabilistic Latent Semantic Indexing (pLSI) [8], LDA has a better statistical foundation by de?ning the topic-document distribution ?, which allows inferencing on new document based on previously estimated model and avoids the problem of over?tting, where both are known as the de?cits of pLSI. Generally, the procedure of generating each word in a document under LDA can be broken down into two stages. One ?rstly chooses a distribution over a mixture of K topics. Following that, one picks up a topic randomly from the topic distribution, and draws a word from that topic according to the topic’s word probability distribution. The existing framework of LDA has three hierarchical layers, where topics are associated with documents, and words are associated with topics. In order to model document sentiments, we propose a joint sentiment/topic (JST) model by adding an additional sentiment layer between the document and the topic layer. Hence, JST is e?ectively a fourlayer model, where sentiment labels are associated with documents, under which topics are associated with sentiment labels and words are associated with both sentiment labels and topics. A graphical model of JST is represented in Figure 1(b). Assume that we have a corpus with a collection of D documents denoted by C = {d1,d2,...,dD}; each document in the corpus is a sequence of Nd words denoted by d = (w1,w2,..., wNd), and each word in the document is an item from a vocabulary index with V distinct terms denoted by {1,2,...,V }. Also, let S be the number of distinct sentiment labels, and T be the total number of topics. The procedure of generating a word wi in document d boils down to three stages. Firstly, one chooses a sentiment label l from the document speci?c sentiment distribution pd. Following that, one chooses a topic randomly from the topic distribution ?l,d, where ?l,d is chosen conditioned on the sentiment label l. It is worth noting at this point that the topicdocument distribution of JST is di?erent from the one of LDA. In LDA, there is only one topic-document distribution ? for each individual document. In contrast, each document in JST is associated with S (number of sentiment labels) topic-document distributions, each of which corresponds to a sentiment label l with the same number of topics. This feature essentially provides means for the JST model to measure the sentiment of topics. Finally, one draws a word from distribution over words de?ned by the topic and sentiment label, which is again di?erent from LDA that a word is sampled from the word distribution only de?ned by topic. The formal de?nition of the generative process which corresponds to the hierarchical Bayesian model shown in Figure 1(b) is as follows: • For each document d, choose a distribution pd ~ Dir(?). • For each sentiment label l under document d, choose a distribution ?d,l ~ Dir(a). • For each word wi in document d – choose a sentiment label li ~ pd, – choose a topic zi ~ ?d,li, – choose a word wi from the distribution over words de?ned by the topic zi and sentiment label li, ?li zi. The hyperparameters a and ß in JST can be treated as the prior observation counts for the number of times topic j associated with sentiment label l sampled from a document and the number of times words sampled from topic j associated with sentiment label l respectively, before having observed any actual words. Similarly, the hyperparameter ? can be interpreted as the prior observation counts for the number of times sentiment label l sampled from document before any words from the corpus is observed. In JST, there are three sets of latent variables that we need to infer, including: the joint sentiment/topic-document distribution ?, the joint sentiment/topic-word distribution ?, and the sentiment-document distribution p. We will see later in the paper that the sentiment-document distribution p plays an important role in determining the document polarity. In order to obtain the distributions of ?, ? and p, we ?rstly estimate the posterior distribution over z, i.e the assignment of word tokens to topics and sentiment labels. The sampling distribution for a word given the remaining topics and sentiment labels is P(zt = j,lt = k|w,z-t,l-t,a,ß,?) where z-t and l-t are vector of assignments of topics and
labels for all the words in the collection except for the word at position t in document d. The joint probability of the topic/sentiment label assignments and the words can be factored into the following three terms:
P(w,z,l) = P(w|z,l)P(z,l) = P(w|z,l)P(z|l,d)P(l|d) (1) For the ?rst term, by integrating out ?, we obtain: P(w|z,l) =G(V ß) G(ß)V T*SY j Y k Qi G(Ni,j,k + ß) G(Nj,k + V ß) (2) where V is the size of the vocabulary, T is the total number of topics, S is the total number of sentiment labels, Ni,j,k is the number of times word i appeared in topic j and with sentiment label k. Nj,k is the number of times words assigned to topic j and sentiment label k, and G is the gamma function. For the second term, by integrating out ?, we obtain: P(z|l,d) =G(Ta) G(a)T S*DY k Y d Qj G(Nj,k,d + a) G(Nk,d + Ta) (3) where S is the total number of sentiment labels, D is the total number of documents in the collection, Nj,k,d is the number of times a word from document d has been associated with topic j and sentiment label k. Nk,d is the number of times sentiment label k has been assigned to some word tokens in document d. For the third term, by integrating out p, we obtain: P(l|d) =G(S?) G(?)SDY d Qk G(Nk,d + ?) G(Nd + S?) (4) where D is the total number of documents in the collection, Nk,d is the number of times sentiment label k has been assigned to some word tokens in document d. Nd is the total number of words in the document collection. Gibbs sampling will sequentially sample each variable of interest, zt and lt here, from the distribution over that variable given the current values of all other variables and the data. Letting the subscript -t denote a quantity that excludes data from tth position, the conditional posterior for zt and lt is:
P(zt = j,lt = k|w,z-t,l-t,a,ß,?) ? {Nwt,j,k}-t+ß {Nj,k}-t+V ß · {Nj,k,d}-t+a {Nk,d}-t+Ta · {Nk,d}-t+? {Nd}-t+S? (5) Equation 5 is the conditional probability derived by marginalizing out the random variables ?, ?, and p. A sample obtained from the Markov chain can be used to approximate the distribution of words in topics and sentiment labels:
?i,j,k =
Ni,j,k + ß Nj,k + V ß
(6)
The approximated predictive distribution over topics for sentiment label is:
?j,k,d =
Nj,k,d + a Nk,d + Ta
(7)
Finally, the approximated predictive distribution over sentiment label for document is:
pk,d =
Nk,d + ? Nd + S?
(8)
The pseudo code for the Gibbs sampling procedure of JST is shown in Figure 2.
1. Initialize V × T × S matrix F, T × S × D matrix T, S × D matrix ?. 2. for m = 1 to M Gibbs sampling iterations do 3. Read a word i from a document 4. Calculate the probability of assigning word i to topic and sentiment label based on Equation 5. 5. Sample a topic j based on the estimated probability obtained in Step 3. 6. Sample a sentiment label k. 7. Update the matrix F, T, and ? with new sampling results. 8. Go to step 3 until all words have been processed. 9. end for
Figure 2: Gibbs sampling procedure.
3.1 Tying-JST Model A variation of JST model is presented in Figure 1(c), namely tying-JST model. The major di?erence between tying-JST and JST model falls into that, in order to sample a word in a document during the generative process, one has to choose a topic-document distribution ?d for every document under the JST model, whereas in tying-JST there is only one topic-document distribution ? which accounts for all the documents in the corpus. Therefore, during the Gibbs sampling procedure, rather than having a T matrix with dimension T×S×D as for JST, the T matrix of tying-JST has only T ×S dimension. As a result, the approximated predictive distribution over topics for sentiment label is di?erent from Equation 7 and should be reformulated as:
?j,k =
Nj,k + a Nk + Ta
(9)
where T is the total number of topics, Nj,k is the total number of times topic j is associated with sentiment label k, and Nk is total number of times that a word is associated with sentiment label k. Experimental results will be presented in Section 5 to compare the performance of the JST and the tying-JST model.
4. EXPERIMENTAL SETUP In this section, we present the experimental setup of document polarity classi?cation and topic extraction based on the movie review dataset. This dataset consists of two categories of free format movie review texts, with their overall sentiment polarity labeled either positive or negative. However, one should note that we do not use any of the polarity label information of the dataset in our experiments but only for evaluating the performance of the JST model, as our model is fully unsupervised.
4.1 Preprocessing Preprocessing was performed on the movie review data before the subsequent experiments. Firstly, punctuation, numbers and other non-alphabet characters were removed. Secondly, for the purpose of reducing the vocabulary size and addressing the issue of data sparseness, stemming was
performed using the Porter’s stemmer algorithm [17]. Stop words were also removed based on a stop word list2. After preprocessing, the corpus contains 2000 documents and 627,317 words with 25,166 distinct terms.
4.2 De?ning Model Priors As has been pointed out by Pang et al. [16], the sentiment classi?cation problem is somehow more challenging than the traditional topic-based classi?cation, since sentiment can be expressed in a more subtle manner while topics can be identi?ed more easily according to the co-occurrence of keywords. One of the directions for improving the sentiment detection accuracy is to incorporate prior information or subjectivity lexicon (i.e., words bearing positive or negative polarity), which can be obtained in many di?erent ways. Some approach annotates polarity to words based on manually constructed Appraisal Groups [24]. Other approach generates subjectivity lexicons in a semi-automatic manner [1]. More recently, Kaji and Kitsuregawa [9] proposed a method which can build polarity-tagged corpus from HTML documents fully automatically. While subjectivity lexicon generation is beyond the scope of this paper, here in our experiments, we investigated incorporating prior information obtained in four di?erent ways into the JST and the tying-JST model, and explored how the prior information can improve the sentiment classi?cation accuracy. Paradigm word list The paradigm word list consists of a set of positive and negative words, e.g. excellent and rubbish. These lexicon words can be simply treated as the paradigms for de?ning the positive and negative semantic orientation, rather than for the purpose of training the algorithm [22]. The majority of the words were derived from the word lists used by Pang et al. [16] for their baseline result tests, with punctuation like ‘?’ and ‘!’ removed. However, we did notice the di?erence that the movie review data used by Pang et al. [16] is an older version with only 700 positive and 700 negative movie reviews, compared to the newer version we used that contains 1000 positive and 1000 negative documents. Hence, we added some additional paradigm words to the original list by reexamining a small portion of the corpus based on a very preliminary check of word frequency counts. Finally, the resulting paradigm word list contains 21 positive and 21 negative paradigm words respectively, as shown in Table 1.
Table 1: Paradigm word list.
Positive dazzling brilliant phenomenal excellent fantastic gripping mesmerizing riveting spectacular cool awesome thrilling moving exciting love wonderful best great superb still beautiful Negative sucks terrible awful unwatchable hideous bad cliched boring stupid slow worst waste unexcit rubbish tedious unbearable pointless cheesy frustrated awkward disappointing
Mutual information (MI) In statistical language modeling, mutual information is a criterion widely used for calculating the semantic association between words. Here we use mutual information to select the words that have strong
2http://ir.dcs.gla.ac.uk/resources/linguistic utils/stop words/
association with positive or negative sentiment classes. The top 20 words within each individual sentiment class were selected based on their MI scores and incorporated as prior information for our models. Full subjectivity lexicon We also explored using the publicly available subjectivity word list with established polarities such as the MPQA subjectivity lexicon3, which consists of 2718 positive and 4911 negative words4. By matching the words in the MPQA subjectivity lexicon with the vocabulary (with 25,166 distinct terms) of the movie review dataset, we ?nally obtained a subset of 1335 positive, 2214 negative words. Filtered subjectivity lexicon The ?ltered subjectivity lexicon was obtained by removing from the full subjectivity lexicon the words occurred less than 50 times in the movie review dataset. The words whose polarity changed after stemming were also removed automatically. Finally, the ?ltered subjectivity lexicon contains 374 positive and 675 negative words. Although one may argue that the paradigm word list and the MI extracted words seem requiring certain supervision information from the corpus itself, the subjectivity lexicon used here is fully domain-independent and does not bear any supervision information speci?cally to the movie review dataset. In fact, the JST model with the ?ltered subjectivity lexicon achieved better performance than the ones using the prior information obtained from paradigm word list or MI extracted words as can be seen later in Section 5. While it is well-known that sentiment classi?ers trained on one domain often fail to produce satisfactory results in another domain, we speculate that the unsupervised nature of our JST model makes it highly portable to other domains.
4.3 Incorporating Prior Information We modi?ed Phan’s GibbsLDA++ package5 for the JST and tying-JST model implementation. In the experiments, the prior information was only utilized during the initialization of posterior distribution z, i.e. assignment of word token to sentiment label and topic. We chose a total number of 3 sentiment labels representing positive, negative and neutral, considering the fact that the sentiment of any word can be categorized into one of these three classes. The initialization starts by comparing each word token in the corpus against the words in the sentiment word list as described in Section 4.2. If there is a match, the word token is assigned with the corresponding sentiment label. Otherwise, a sentiment label is randomly sampled for a word token.
5. EXPERIMENTAL RESULTS In this section, we will present and discuss the experimental results of both document sentiment classi?cation and topic extraction, based on the movie review dataset.
5.1 Sentiment Classi?cation The document sentiment is classi?ed based on P(l|d), the probability of sentiment label given document, which is approximated using Equation 8 in the implementation. In our 3http://www.cs.pitt.edu/mpqa/ 4We discarded words with ‘neutral’ label in the subjectivity lexicon since the number of neutral words is small and many of the neutral words have multiple polarities, e.g. both neutral and positive. 5http://gibbslda.sourceforge.net/
Table 2: Results of incorporating various prior information. # of polarity words JST (%) Tying-JST (%) Prior information (pos./neg.) pos. neg. overall pos. neg. overall Without prior information 0/0 63 56.6 59.8 59.2 53.8 56.5 Paradigm words 21/21 70.8 77.5 74.2 74.2 71.3 73.1 Paradigm words + MI 41/41 76.6 82.3 79.5 78 73.1 75.6 Full subjectivity lexicon 1335/2214 74.1 66.7 70.4 77.6 69 73.3 Filtered subjectivity lexicon 374/675 84.2 81.5 82.8 84.6 73.1 78.9 Filtered subjectivity lexicon (subjective MR) 374/675 96.2 73 84.6 89.2 74.8 82 Pang et al. (2002) [16] N/A Classi?er used: SVMs Best accuracy: 82.9% Pang and Lee (2004) [15] (subjective MR) N/A Classi?er used: SVMs Best accuracy: 87.2% Whitelaw et al. (2005) [24] 1597 appraisal groups Classi?er used: SVMs Best accuracy: 90.2% Kennedy and Inkpen (2006) [10] 1955/2398 Classi?er used: SVMs Best accuracy: 86.2%
experiments, we only consider the probability of positive and negative label given document, with the neutral label probability being ignored. There are two main reasons. Firstly, movie review sentiment classi?cation in our case is e?ectively a binary classi?cation problem, i.e. documents are being classi?ed either as positive or negative, without the alternative of neutral. Secondly, the prior information we incorporated merely contributes to the positive and negative words, and consequently there will be much more in?uence on the probability distribution of positive and negative label given document, rather than the distribution of neutral label given document. Therefore, we de?ne that a document d is classi?ed as a positive-sentiment document if its probability of positive sentiment label given document P(lpos|d), is greater than its probability of negative sentiment label given document P(lneg|d), and vice versa. In this section, we show how prior information improves the sentiment classi?cation accuracy of the JST and tyingJST models and how topic mixtures a?ect the performance of our models.
5.1.1 Results with Different Prior Information Table 2 shows the sentiment classi?cation accuracy at document level by incorporating various prior information. The number of polarity (positive and negative) words in various subjectivity word list is also listed. In all of the results showed in the table, a is set to 50 #topics, ß is set to 0.01. It should be noted that while LDA can produce reasonable results with a simple uniform Dirichlet prior for its hyperparameters, asymmetric prior ? for sentiment-document distribution should be used since it captures di?erent correlations among sentiment labels. In our experiments, ? is set to 0.01 for positive sentiment label and 5 for negative sentiment label. The setting for ? was determined empirically. It is worth pointing out that hyperparameters can be learned from data directly by maximum likelihood or maximum a posteriori estimation [23]. Alternatively, an approximation approach such as moment matching could also be used to avoid iterative methods for the sake of simplicity and speed [12]. We leave the estimation of ? in a more principled way as future work. It can be observed from Table 2 that without incorporating any prior information, JST only achieved around 60% overall accuracy. By incorporating merely 21 positive and 21 negative paradigm words, a signi?cant performance improvement is observed with JST and tying-JST giving an overall of 74.2% and 73.1% accuracy respectively. We also
experimented the combination of paradigm words and mutual information and evaluated how mutual information can help to improve the sentiment classi?cation accuracy. We extracted the top 20 positive/negative words based on the MI value calculated from the 40 randomly selected labeled documents from the movie review dataset with equal number of positive and negative documents. Plus the paradigm words listed in Table 1, the total number of positive and negative words is 41 each. It can be observed that there is a considerable improvement in classi?cation accuracy after incorporating the MI-extracted words, with 5.3% and 2.5% improvement for JST and tying-JST respectively. Subjectivity lexicons have attracted increasing focus in previous work [1]. Intuitively, one might expect that with a larger subjectivity lexicon and hence an increasing number of polarity words, sentiment classi?cation performance would be improved since an overall polarity of a text can be inferred from the aggregated polarity of its individual words. However, the results shown in Table 2 reveal that incorporating the full subjectivity lexicon with 1335 positive and 2214 negative words in fact hurts the performance of both JST and tying-JST, with a relatively poor overall accuracy of 70.4% and 73.3% being achieved respectively. In contrast, with the ?ltered subjectivity lexicon by removing the infrequent polarity words, the performance of both models improves. Thus, the full subjectivity lexicon actually introduces more noise into the models and hence resulted in poorer performance. Also, the yielding results (82.8% for JST and 78.9% for tying-JST) are actually better than the performance by incorporating any aforementioned prior information. We also observe that tying-JST performed consistently worse than the JST model except for the case of incorporating full subjectivity lexicon as prior information. Therefore, JST seems to be a more reasonable model design in terms of sentiment classi?cation.
5.1.2 Results with Subjectivity Detection In another set of experiments, we followed the approach in [15] and performed subjectivity detection (with sentences that do not express any opinions removed) prior to sentiment classi?cation. Subjective sentences were extracted from the original movie review dataset using the LingPipe package6. First, we trained the subjectivity classi?er based on the Sub
6http://alias-i.com/lingpipe/demos/tutorial/sentiment/readme.html
5060701 50 100Accuracy NumberoftopicsWithoutpriorinformationPositiveaccuracyNegativeaccuracyOverallaccuracy (a)
7585curacy ParadigmwordsPositiveaccuracyNegativeaccuracyOverallaccuracy556575851 50 100Accuracy NumberoftopicsParadigmwordsPositiveaccuracyNegativeaccuracyOverallaccuracy (b)
708090curacy Paradigmwords+MIPositiveaccuracyNegativeaccuracyOverallaccuracy50607080901 50 100Accuracy NumberoftopicsParadigmwords+MIPositiveaccuracyNegativeaccuracyOverallaccuracy (c)
506070801 50 100Accuracy NumberoftopicsFullsubjectivitylexiconPositiveaccuracyNegativeaccuracyOverallaccuracy (d)
6575851 50 100Accuracy NumberoftopicsFilteredsubjectivitylexiconPositiveaccuracyNegativeaccuracyOverallaccuracy (e)
90100curacyFilteredsubjectivitylexicon(subjectiveMR)PositiveaccuracyNegativeaccuracyOverallaccuracy7080901001 50 100Accuracy NumberoftopicsFilteredsubjectivitylexicon(subjectiveMR)PositiveaccuracyNegativeaccuracyOverallaccuracy (f) Figure 3: Sentiment classi?cation accuracy VS. di?erent topic numbers. jectivity v1.0 dataset7 which contains 5000 subjective and 5000 objective sentences. The trained classi?er was then used to extract the subjective sentences from the movie review dataset, which reduces each single document to 5 to 25 sentences. After subjectivity detection and data preprocessing as described in Section 4.1, the dataset, which we named as“subjective MR”, still contains 2000 documents but with a total of 334,336 words and 18,013 distinct terms (c.f. 25,166 distinct terms without subjectivity detection). It can be seen from Table 2 that the best performance for both JST and tying-JST is obtained on the subjective MR dataset with the prior sentiment label information obtained from the ?ltered subjectivity lexicon, where an overall accuracy of 84.6% and 82% was achieved by JST and tyingJST respectively. This is a clear improvement over 82.8% and 78.9% when no subjectivity detection was performed. It suggests that though the subjective MR dataset is in a much compressed form, it is more e?ective than the full dataset as it retains comparable polarity information in a much cleaner way [15].
5.1.3 Comparison with Existing Approaches For comparison, document-level sentiment classi?cation results on the movie review dataset from four previous studies are also listed in the last four rows of Table 2. The best result reported in [16] is 82.9%, which is attained by support vector machines (SVMs) using bag-of-unigram features. The performance was later further improved to 87.2% [15] by applying SVMs on the subjective portions of the movie reviews which were extracted using a subjectivity detector as described in Section 5.1.2. Whitelaw et al. [24] used SVMs to train on the combination of di?erent types of appraisal group features and the bag-of-words features for sentiment analysis. The reported best accuracy is 90.2% using 1,597 appraisal groups with each possible combination of Attitude and Orientation plus 48,314 bag-of-words features. Their
7http://www.cs.cornell.edu/People/pabo/movie-reviewdata/
appraisal groups were constructed semi-automatically and comprise of a total of 41,082 appraising groups. This is much more complicated than the subjectivity lexicon used in this paper. Kennedy and Inkpen [10] combined two main sources, General Inquirer8 and Choose the Right Word [7], to obtain a total of 1,955 positive and 2,398 negative terms. They then trained two classi?ers, one was based on counting the number of positive and negative terms contained in movie reviews and augmented with contextual valence shifters, while the other was based on SVMs trained from the combination of unigrams and valence shifter bigrams. These two classi?ers were ?nally combined to give the best classi?cation accuracy which is 86.2%. In our experiment, the best overall accuracy achieved by JST is 84.6%, based on the ?ltered subjectivity lexicon and the subjective MR dataset. It outperforms the best result reported in [16] and is only 2.6% and 1.6% lower than the results reported in [15] and [10]. Even for the state-of-theart result reported in [24], the best accuracy achieved by JST is only 5.6% lower. While all the previous studies mentioned here relied on the labeled movie review data to train sentiment classi?ers, our proposed JST model is fully unsupervised. In addition, the previous reported results [15, 24, 10] were all based on 10-fold cross validation in a test set comprising of 200 documents only9, our experimental results reported here are based on the whole movie review dataset with a total of 2000 documents.
5.1.4 Results with Different Topics We also evaluated the mixture of topics and sentiments. Figure 3 shows the sentiment classi?cation accuracy of the JST model incorporating prior information obtained in different ways with the number of topics set to 1, 50 and 100. When the topic number is set to 1, the JST model is es
8http://www.wjh.harvard.edu/~inquirer/ 9[16] used an early version of the movie review data which consists of 700 positive and 700 negative documents and the results were based on 3-fold cross validation.
Table 3: Example of topics extracted by JST under di?erent sentiment labels. Positive sentiment label Negative sentiment label Topic 1 Topic 2 Topic 3 Topic 1 Topic 2 Topic 3 w P(w|z,l) w P(w|z,l) w P(w|z,l) w P(w|z,l) w P(w|z,l) w P(w|z,l) good 0.084708 tom 0.035175 ship 0.059020 bad 0.079132 sex 0.065904 prison 0.073208 realli 0.046559 ryan 0.030281 titan 0.031586 worst 0.035402 scene 0.053660 evil 0.032196 plai 0.044174 hank 0.025388 crew 0.024439 plot 0.033687 sexual 0.031693 guard 0.031755 great 0.036645 comedi 0.021718cameron0.024439 stupid 0.029767women0.026291 green 0.029109 just 0.028990 star 0.020800 alien 0.022826 act 0.025602 rate 0.023770 hank 0.028227 perform 0.028362 drama 0.016519 jack 0.020751 suppos 0.025480 act 0.023230 wonder 0.027345 nice 0.026354 meg 0.015601 water 0.019137 script 0.024500 o?ens 0.018728 excute 0.026904 fun 0.025978 joe 0.014378 stori 0.017984 wast 0.024500 credict 0.016027 secret 0.025581 lot 0.025853relationship0.014072 rise 0.016601 dialogu 0.023643 porn 0.014587 mile 0.022936 act 0.022715 mail 0.013766 rose 0.013835 bore 0.022908 rape 0.013867 death 0.022495 direct 0.021586 blond 0.013460 boat 0.013374 poor 0.022908 femal 0.013686 base 0.022054 best 0.020331 run 0.012543 deep 0.013143 complet0.020825 cut 0.013686 tom 0.019849 get 0.020331 phone 0.012237 ocean 0.012451 line 0.019968 gril 0.013506 convict 0.018967 entertain0.018198 date 0.011931 board 0.011990 terribl 0.018988 parti 0.012426 return 0.018526 better 0.017445 got 0.011625 sink 0.011299 mess 0.015313 male 0.011886franklin0.016762 job 0.016692 busi 0.011319 sea 0.010838 wors 0.014333 bad 0.011346 happen 0.016321 talent 0.016064 cute 0.011013 rain 0.010838 dull 0.013598 nuditi 0.011166 power 0.014116 pretti 0.016064 sister 0.010708 dicaprio 0.010607 actor 0.012986woman0.010986 known 0.012352 try 0.015688 children 0.010096 storm 0.010377 total 0.012986 peopl 0.010986 instinct0.011470 want 0.015186 dog 0.009790 disast 0.010146 isn 0.012863 nake 0.010625 inmat 0.011470
sentially transformed to a simple LDA model with only S topics, each of which corresponds to a sentiment label. Consequently, it ignores the correlation between sentiment labels and topics. It can be observed from Figure 3 that, JST performs worse with single topic compared to 50 and 100 topics, except for the case of full subjectivity lexicon as shown in Figure 3(d) where the single topic performance is almost the same as the one with 100 topics. For paradigm words + MI, ?ltered subjectivity lexicon and ?lter subjectivity lexicon (subjective MR) (Figures 3(c), 3(e), and 3(f)), the result with 100 topics outperforms the ones with other topic number settings. For the case when no prior information is applied as well as paradigm words as shown in Figure 3(a) and Figure 3(b), the results with 50 topics are almost the same as the ones achieved with 100 topics and both are higher than that of the single topic setting. It can be also easily seen that the results with ?ltered subjectivity lexicon in Figure 3(e) give the most balanced classi?cation accuracy on both positive and negative documents. From the above, we can conclude that topic information indeed helps in sentiment classi?cation as the JST model with the mixture of topics consistently outperforms a simple LDA model ignoring the mixture of topics. This justi?es the proposal of our JST model. Also, the empirical results reveal that the optimum number of topics for the movie review dataset is 100.
5.2 Topic Extraction The second goal of JST is to extract topics from the movie review dataset (without subjectivity detection) and evaluate the e?ectiveness of topic sentiment captured by the model. In the experiment, the distribution of words given topic and sentiment label was estimated using Equation (6). Unlike the LDA model that a word is drawn from the topic-word distribution, in JST one draws a word from the distribution over words conditioned on both topics and sentiment labels. Therefore, we analyze the extracted topics under two di?er
ent sentiment labels (positive and negative). Six example topics extracted from the movie review dataset under positive and negative sentiment labels are shown in Table 3. The three topics on the left columns of Table 3 were generated under the positive sentiment label and the remaining topics were generated under the negative sentiment label, each of which is represented by the top 20 topic words. As can be seen from the table that the six extracted topics are quite informative and coherent, where each of them tried to capture the underlying theme of a movie or the relevant comments from a movie reviewer. For example, under the positive sentiment label category, topic 1 is likely to be very positive review comments for a movie; topic 2 is apparently about the movie“You’ve got a mail”by Tom Hanks and Meg Ryan; topic 3 is closely related to the very popular romantic movie “Titanic” directed by James Cameron and casted by Leonardo DiCaprio and Kate Winslet. For the topics under the negative sentiment category, topic 1 is probably the criticism made by a movie reviewer, while topic 2 is about movies related to sex/porn issues and topic 3 is likely to be the movie “Green Mile”by Tom Hanks. In terms of topic sentiment, by examining each of the topics in Table 3, it is quite evident that topic 1 under the positive sentiment label and topic 1 under the negative label indeed bear positive and negative sentiment respectively. For topic 2 and topic 3 under the negative sentiment label, it is still fairly easy to recognize that some of their topic words convey negative sentiments though not as strong as the ones in topic 1. Topic 2 and topic 3 under the positive sentiment label mainly describe movie plots with less words carrying positive sentiment compared to topic 1 under the same category. Manually examining the data reveals that the terms that seem not conveying sentiments under these two topics in fact appear in the context expressing positive sentiments. The above analysis illustrates the e?ectiveness of JST in extracting mixture of topics from a corpus.
6. CONCLUSIONS AND FUTURE WORK In this paper, we have presented a joint sentiment/topic (JST) model which can detect document level sentiment and extract mixture of topics from text simultaneously. In contrast to most of the existing approaches in sentiment classi?cation which rely on supervised learning, the proposed JST model is fully unsupervised, thus provides more ?exibilities and can be easier adapted to other applications. Experiments have been conducted to evaluate the performance of JST based on the movie review dataset. The preliminary results demonstrated that our model is able to give competitive performance in document level sentiment classi?cation compared with the results generated by other existing supervised approaches and the discovered topics are indeed coherent and informative. One of the limitations of our model is that it represents each document as a bag of words and thus ignores the word ordering. It will probably predict the sentiment of“not good movie” being positive and the sentiment of “not bad movie” being negative. Thus, in future work, we will extend the model to include higher order information (bigrams or trigrams). Another promising future step is to extend JST to detect the polarity of text at various granularity levels, e.g. detecting sentiment labels for more ?ne-grained topics. We also intend to carry out a large scale of experiments and evaluate the model performance on datasets from di?erent domains.
7. ACKNOWLEDGMENTS We thank Rui Wang and Naihui He for processing part of the prior information and Lei Wang for providing computing resources.
